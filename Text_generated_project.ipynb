{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mr</th>\n",
       "      <th>ref</th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>name[nameVariable], food[Chinese], familyFrien...</td>\n",
       "      <td>it's a Chinese place, also nameVariable is nea...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  mr  \\\n",
       "0  name[nameVariable], food[Chinese], familyFrien...   \n",
       "\n",
       "                                                 ref  period  \n",
       "0  it's a Chinese place, also nameVariable is nea...       2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('sentence_scoping_train.csv',index_col=False)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'a': 2, 'it': 3, 'namevariable': 4, 'rating': 5, 'friendly': 6, 'in': 7, 'and': 8, 'near': 9, 'nearvariable': 10, 'place': 11, \"it's\": 12, 'also': 13, 'has': 14, 'an': 15, 'restaurant': 16, 'riverside': 17, 'with': 18, 'family': 19, 'kid': 20, 'average': 21, 'coffee': 22, 'shop': 23, \"isn't\": 24, 'pub': 25, 'italian': 26, 'excellent': 27, 'chinese': 28, 'city': 29, 'centre': 30, 'high': 31, 'moderately': 32, 'priced': 33, 'fast': 34, 'food': 35, 'expensive': 36, 'french': 37, 'japanese': 38, 'mediocre': 39, 'decent': 40, 'cheap': 41, 'the': 42, 'price': 43, 'range': 44, 'of': 45, '£20': 46, '25': 47, 'low': 48, 'indian': 49, 'english': 50}\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "txt = list(data['ref'][0:50])\n",
    "text = ' '.join(str(e) for e in txt)\n",
    "\n",
    "#print(text[0:200])\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "corpus = text.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "42\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index['in'])\n",
    "print(tokenizer.word_index['the'])\n",
    "print(tokenizer.word_index['of'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 1, 'a': 2, 'it': 3, 'namevariable': 4, 'rating': 5, 'friendly': 6, 'in': 7, 'and': 8, 'near': 9, 'nearvariable': 10, 'place': 11, \"it's\": 12, 'also': 13, 'has': 14, 'an': 15, 'restaurant': 16, 'riverside': 17, 'with': 18, 'family': 19, 'kid': 20, 'average': 21, 'coffee': 22, 'shop': 23, \"isn't\": 24, 'pub': 25, 'italian': 26, 'excellent': 27, 'chinese': 28, 'city': 29, 'centre': 30, 'high': 31, 'moderately': 32, 'priced': 33, 'fast': 34, 'food': 35, 'expensive': 36, 'french': 37, 'japanese': 38, 'mediocre': 39, 'decent': 40, 'cheap': 41, 'the': 42, 'price': 43, 'range': 44, 'of': 45, '£20': 46, '25': 47, 'low': 48, 'indian': 49, 'english': 50}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 894 samples\n",
      "Epoch 1/10\n",
      "894/894 [==============================] - 93s 104ms/sample - loss: 3.2464 - accuracy: 0.2036\n",
      "Epoch 2/10\n",
      "894/894 [==============================] - 87s 98ms/sample - loss: 1.8853 - accuracy: 0.4575\n",
      "Epoch 3/10\n",
      "894/894 [==============================] - 87s 97ms/sample - loss: 1.3774 - accuracy: 0.5347\n",
      "Epoch 4/10\n",
      "894/894 [==============================] - 87s 97ms/sample - loss: 1.1879 - accuracy: 0.5817\n",
      "Epoch 5/10\n",
      "894/894 [==============================] - 88s 99ms/sample - loss: 1.0293 - accuracy: 0.6242\n",
      "Epoch 6/10\n",
      "894/894 [==============================] - 86s 96ms/sample - loss: 0.8796 - accuracy: 0.6655\n",
      "Epoch 7/10\n",
      "894/894 [==============================] - 87s 97ms/sample - loss: 0.7244 - accuracy: 0.7304\n",
      "Epoch 8/10\n",
      "894/894 [==============================] - 87s 97ms/sample - loss: 0.5832 - accuracy: 0.7830\n",
      "Epoch 9/10\n",
      "894/894 [==============================] - 86s 97ms/sample - loss: 0.4181 - accuracy: 0.8468\n",
      "Epoch 10/10\n",
      "894/894 [==============================] - 86s 97ms/sample - loss: 0.3135 - accuracy: 0.9027\n",
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x00000178748D1848>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(xs, ys, epochs=10, verbose=1)\n",
    "#print model.summary()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"it's a Chinese place, also nameVariable is near nearVariable. it is family friendly. it's near\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
